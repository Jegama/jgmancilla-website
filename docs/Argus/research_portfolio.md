# Research Portfolio:

## Project: Quant UX Research at Scale
Short Description: Reduced open‑ended survey analysis time ~30h → <8h and doubled throughput by deploying a hybrid ML + human‑in‑loop classifier and integrated behavioral log triangulation.
Situation: Open‑ended survey analysis (multi-project, bi‑weekly cadence) was a bottleneck for product decisions—manual coding cycles (~30h) slowed insight delivery for a 500M+ MAU product.
Task: Accelerate qualitative signal extraction while preserving nuance and statistical rigor; establish a scalable, governable pipeline that stakeholders trust.
Actions: Architecture & Automation: Engineered a hybrid classifier combining clustering analysis, few‑shot prompts, and multi‑agent chain‑of‑reasoning to categorize open‑ended responses., Implemented Python toolchain (regressions, ANOVA, T‑tests) to standardize post‑classification quantitative validation. | Human‑in‑Loop Quality: Embedded SME acceptance tests and feedback loops to iteratively refine label taxonomies and guard against semantic drift., Merged editor log behavioral data with survey outputs to triangulate emergent patterns and surface actionable discrepancies. | Scalability & Adoption: Modularized pipeline components (prompt templates, clustering params, evaluation scripts) enabling reuse across adjacent studies., Documented workflows + onboarding guide, enabling fellow researchers to self‑serve advanced analysis routines.
Results: Cycle time reduced from ~30h manual coding to <8h automated + review.; Doubled survey analysis throughput without adding headcount.; Sustained insight quality via embedded statistical validation + SME acceptance criteria.; Unlocked faster iteration on a 500M+ MAU product by shortening feedback loops.
Technologies: Python, Clustering Analysis, Few‑Shot Learning, Multi‑Agent Reasoning, Human‑in‑Loop Feedback, Statistical Testing (ANOVA / T‑tests / Regressions), Behavioral Log Integration, Survey Analytics Pipeline
Metrics: Analysis time: ~30h → <8h (~73% reduction); Throughput: 2× surveys processed per cycle; Product scale: 500M+ MAU context; Bi‑weekly longitudinal cadence maintained

## Project: Design Research
Short Description: Real‑world driver stress elicitation: replicated 90% stress induction & 89% event correlation via mixed-methods instrumentation.
Situation: Need to reliably elicit and measure driver stress in naturalistic on‑road conditions to study behavioral and physiological responses.
Task: Design a safe, repeatable stress elicitation protocol and instrumentation stack; validate detection algorithm and participant experience.
Actions: Protocol & Instrumentation: Combined scripted (battery warnings) and unscripted road stressors (pedestrians, construction) to broaden ecological validity., Implemented experimental low‑battery display producing escalating visual & auditory prompts mid‑drive. | Data & Validation: Captured psychophysiological signals + participant self‑reports to triangulate stress windows., Released an open API for the stress window detection algorithm to enable reproducibility. | Collaboration: Maintained continuous cross‑disciplinary communication ensuring safety, experimental rigor, and participant comfort.
Results: 90% of participants experienced measurable stress response.; 89% of detected stress windows aligned with observed road events, supporting construct validity.; Established a reusable, field‑tested methodology enabling future comparative driver studies.
Technologies: Wearable Sensors, Custom In‑Vehicle Displays, Self‑Report Surveys, Data Analysis Algorithms
Metrics: Stress replication rate: 90%; Event correlation accuracy: 89%

## Project: User Research
Short Description: Framework + longitudinal HW usability evaluation driving remote redesign & metric expansion across millions of devices.
Situation: Multiple hardware remote control variants in market with unclear comparative usability and evolving feature sets across 70M+ active devices.
Task: Establish statistically robust benchmarking framework; validate device & country‑level behavioral metrics to guide redesign decisions.
Actions: Framework & Data Pipeline: Co‑developed baselines with senior researcher; iteratively refined tasks via repeated usability studies., Integrated behavioral log aggregation with scripted lab findings for holistic usage modelling. | Metric Evolution: Added country‑specific and feature‑exclusive metrics as new remote capabilities emerged., Developed comparative dashboards enabling rapid cross‑device evaluation. | Stakeholder Alignment: Collaborated with designers & PMs to prioritize metrics tied to decision inflection points., Scripted statistical analyses for repeatability and reduced turnaround across the two‑year span.
Results: Informed launch of new remote (Fall '22) with validated interaction improvements.; Expanded metric suite increased diagnostic granularity for regional & feature differentiation.; Documentation + scripted analysis preserved continuity across intermittent project phases.
Technologies: Usability Testing (field/lab/remote), Behavioral Log Analysis, Surveys & Interviews, Statistical Analysis Scripts, Dashboards
Metrics: Scope: 70M+ device usage signals integrated; Multi‑year longitudinal framework (≈2 years)

## Project: Research Ops
Short Description: Progressive rollout of standardized repository & reporting boosted visibility and self‑serve insights org‑wide.
Situation: High‑value research outputs under‑leveraged due to dispersed documents, inconsistent reporting, and limited cross‑team visibility.
Task: Centralize knowledge, standardize documentation, and drive adoption without imposing disruptive process overhead.
Actions: Platform & Standardization: Introduced structured templates and consolidated prior scattered reports into a searchable repository., Instituted visualization layer surfacing project status for leadership triage. | Adoption Strategy: Pilot with immediate team positioned as low‑risk trial; iteratively showcased early wins to expand uptake., Enabled self‑serve discovery reducing ad‑hoc request load on researchers.
Results: Repository evolved into self‑serve knowledge base increasing reuse & reducing redundant inquiry.; Improved cross‑team collaboration and leadership visibility into portfolio health.; Lower friction onboarding to process changes via phased rollout approach.
Technologies: Knowledge Management Systems, Visualization Tools, Research Templates, Documentation Platforms
Metrics: Adoption: Successful expansion from pilot to org‑wide usage (qualitative milestone); Process: Reduced manual status update cycles (anecdotal leadership feedback)
